The process of machine learning
=====================

1. Know your problem
2. Know your data (is it text, is it image)
3. Set up your test harness
4. Spot check a variety of algorithms
5. Fine tune the best ones

## Know your problem

The most fundamental question for machine learning (or indeed most projects) is what problem are you trying to solve? Can machine learning help you with this?  What kind of machine learning question is it?

- What problem are you trying to solve?
- Is machine learning a good fit for this problem?
  - Is this a supervised or unsupervised question?
- Should you use an existing simple model instead?

<!-- TODO understanding your problem -->


### Key data concepts

*Items*, *Points*, and *Samples* are terms for the individual items which you will be learning from or processing.  Samples implies a selection of items for a given purpose, but the terms are largely interchangable.

*Features* are the distinct traits associated with each sample.  

*Labels* are what you are attempting to predict for each sample.  *Classes* are the discrete possible values which labels may take.  


- Samples
- Patterns
- Labels

### Main types of learning

- Supervised: make predictions
  - Classification: predict from a sample of labels
  - Regression: predict a numeric value
- Unsupervised: identify structure
  - Clustering: find groups within your data
  - Density estimation: identify distribution of samples in the feature domain
  - Pattern identification: identify common patterns
  - Representation learning: describe the data with much less information
- Semi-supervised: supervised learning with additional unlabeled data to help characterize the feature domain
- Reinforcement learning: data is generated by interaction with environment, attempting to maximize a reward

## Know your data

You must also understand your data and how to use it.

- Where is your data coming from?
  - How will you obtain it?
  - Do you have sufficient privileges?
  - Are you legally allowed to use your data to solve your problem? Be especially cautious about patient data.  
- What form is your data in?
  - How will you wrangle it into a form that can be analyzed?
  - Do you need to wrangle your data only once or more often?
  - What are the types of your features?
  - Will any of them need to be transformed into a different form for processing?
- What does your data look like?
  - Can you plot it and if so, is their a pattern?
- What data do you have to work with?
  - Do I have enough examples to train with?
  - Do you have many more samples than features? If not training may not be effective.  
  - Do you have a lot of irrelevant features? These can introduce spurious correlations.  
  - Are there any other features that you think might be useful and can you get them?
  - Is it labeled, and if so how?
  - Are your labels balanced?
  - Are any of your features exponential in nature? You may want to log transform them.  
  - Are any of the features in your data identifiers? You may want to discard these: they can be used to overfit, and should not be interpreted as numbers.  

## Set up your test harness

The main steps are as follows:

1. Wrangle your data (e.g. from text)
2. Preprocess your feature set (remove bad or useless features, engineer new features)
3. Pick performance measure (usually accuracy)
4. Spot check models with cross validation and evaluate scores.  

### Data wrangling

Data wrangling (or munging or preparation) is widely considered to be the hardest, most complicated part of ML.  There are many aspects of this problem:

  - May come in a wide variety of formats
  - May be messy and include missing data
  - May include unimportant data
  - May be too large for memory, requiring special techniques to process

In many if not most cases, the amount of time you spend wrangling your data will exceed the amount of time you spend actually choosing and fitting models.  As trivial as it sounds, you can't train a model without data.

### Performance measure

### Cross Validation

Cross validation requires you to split your training data you have available to you into two sets, train and cv.  You train with the train set and then evaluate on the cv set.  Because the model you trained knows nothing about the cv set, your cv score is a good indicator about how well your model will perform on future data.  The train score tells you only how well you were able to model the data you trained with.  

There are no hard-and-fast guidelines about how train and test splits should be made, what the relationship of the splits should be, or whether to use the same splits on all models.  Cross-fold validation (splitting your data into 3 or more equal sized sets, and test with each one while training with the others) is a popular approach.  The larger the number of folds, the more accurate your aggregate score will be (the extreme case is leave-one-out, where you do this for each individual data point).  Generally during model exploration I prefer to test 1-3 80/20 random splits, and add more if I decide to investigate further.  This seems to work fine.  

### Preprocessing

Most algorithms are only capable of processing numeric values.  Therefore any non-numeric features must be converted:

- Binary features are usually converted to 0 and 1.  
- Categorical features with N values generally should be expanded to N binary features.  This is known as one-hot encoding.  
- Text features are either discarded, treated as categorical features, or otherwise converted to a numeric representation.  It depends on the circumstances.  

Several algorithms (including all the linear ones) require features in models to have similar distributions.  This usually means they are standardized (equal means and equal variances) or restricted to a range (usually (0, 1) or (-1, 1)).  However, such scaling ensures that all features will have equal relevance.  If your features already are scaled meaningfully compared to each other (important features are bigger), then scaling may actually hurt your ability to generate a good model.  

Other models perform optimally with whitening: not only scaled, but features should not correlate with each other.  In those cases, you may need to apply more complicated algorithms such as PCA or LDA.  

To ensure honest cv scores, your preprocessing step should only use the train set.  In other words, do _not_ scale your data set and then pick your train set from that, pick your train set and then scale based on that.  So for each model you will need to:

1. Scale or whiten your training set, if necessary
2. Scale or whiten your test set using the parameters that you used on your train set
2. Train model on your preprocessed training data
3. Score your preprocessed train and test data with your model and performance measure

In other circumstances you may want to perform relatively heavy preprocessing such as PCA or LDA on the entire data set, possibly using unlabeled data entirely outside of your training set.  Although this will allow you to evaluate more models in less time and may boost your cv score, it may not be representative of how the model will perform on future unknown data.  

Some machine learning libraries such as scikit-learn provide a Pipeline object, which you can use to combine your preprocessor and model into a single object that behaves like a model for convenience in training and processing.  Other packages may have built-in handling of preprocessing.  

### Algorithms to spot check

I would strongly recommend that you always spot check the following models with the specified parameters

- Extremely Randomized Trees:
- Random Forest:
- Linear or Logistic Regression: (standardized data)
- Neural Networks
  - Extreme Learning Machines (hidden layer with three times as many nodes as features)
  - Basic 1 and 2-layer perceptrons (hidden layers with half as many nodes as features)

## Evaluating your models

- If your train score is good but your test score is poor
  - You are overfitting.  Adjust your model parameters to try to generalize better.  
- If your train and test scores are poor, then your model is not fitting for any one of the following reasons:
  - Try different hyperparameters.
  - Your model may not have yet converged.  Train for more iterations, and see if it is improving.  
  - The intrinsic bias of your model may be poorly suited to the form of your data.  Try different models.  
  - The features you are working with are not good.  
  - The data you are trying to train with may be of poor quality and it is not possible to generate a good model on top of it (garbage in, garbage out).  Examine your data and see if you need to perform any preprocessing or correction on any of the features.  
- If you are running out of memory or your computer is excessively hot or your hard drive is noisy (indicating paging of memory from disk to RAM)
  - Your model may not handle large numbers of features well.  Try reducing your features and see if that helps.  
  - Your model may not handle large numbers of samples well.  Try training your algorithm in batches (if possible), or reduce the batch size.
  - You may be trying to process more data than your system can handle.  Try buying or using a system with more RAM.
- If your model fails to train at all, or is training very slowly or poorly:
  - Verify that your input was preprocessed in the form required by the model
  - One of your hyperparameters may be be making your model diverge.  Try different hyperparameters.

## Refining your models

Now that you have some good candidate models, see if you can improve their performance by adjusting their hyperparameters.  Generally you can and should do this using a grid search of hyperparameter combinations, or a randomized search within a certain window.  

If your models are nowhere near as good as you think they could be, try performing some feature engineering and see if that improves the result.  Most algorithms do not generate products of features, which can greatly improve models in some instances.  You can help this along by introducing combinatorial features (products of two or more features).  Alternatively, in linear algorithms you can log-transform features: since the sum of logs is equal to the log of products, products and exponents can be discovered.  

- Extreme Learning Machine style random non-linear mapping
- PCA or LDA dimension reduction

### Super learners

If you have multiple models that perform well, you can combine their output predictions weighted by their cross-validation accuracy.  The result will probably better than the sum of its parts.  
