<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>A Practical Guide to Applied Machine Learning</title>

  <meta name="description" content="Using python and scikit-learn, we'll walk through a machine learning process from start to finish discussing when to apply machine learning, the most successful algorithms, best practices, and even a little philosophy.">
  <meta name="author" content="Jeremy Gore">

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <link rel="stylesheet" href="../reveal.js/css/reveal.css">
  <link rel="stylesheet" href="../css/tessella.css" id="theme">
  <link rel="stylesheet" href="../css/presentation.css">

  <!-- Code syntax highlighting -->
  <link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement('link');
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match(/print-pdf/gi) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
    document.getElementsByTagName('head')[0].appendChild(link);
  </script>

<!--[if lt IE 9]>
<script src="lib/js/html5shiv.js"></script>
<![endif]-->
</head>

<body>
<div class="reveal">
<div class="slides">

<section>
  <section id="title" class="container">
    <div class="col-md-4 nopadding">
      <svg width="300" height="600"></svg>
    </div>
    <div class="col-md-8 text-right">
      <img class="noborder" style="width:300px;height:auto; margin-bottom:1em; " src='../img/Tessella_Logo.png'>
      <h2 class="text-right">A Practical Guide to Applied Machine Learning</h2>
      <h2 class="text-right"><small>Jeremy Gore<br>Autumn Conference 2015-10-05</small></h2>
    </div>

    <aside class="notes">
      <p>To say machine learning is a big subject is a big understatement,
        but it may be easier than you think.</p>
      <p>Using python and scikit-learn, we'll walk through a machine learning
        process from start to finish discussing when to apply machine learning,
        the most successful algorithms, best practices, and even a little
        philosophy.</p>
    </aside>
  </section>

  <section id="instructions" data-background="#996699" data-background-transition="zoom">
    <h3>Presentation instructions</h3>
    <ul>
      <li>Press <kbd>ESC</kbd> to enter the slide overview</li>
      <li>Press <kbd>F</kbd> to display the speaker view with notes</li>
      <li>Press <kbd>B</kbd> or <kbd>.</kbd> to pause the presentation</li>
      <li>Press <kbd>S</kbd> to display the speaker view with notes</li>
      <li><kbd>Alt+click</kbd> on any element to zoom in on it, <kbd>Alt+click</kbd> anywhere to zoom back out</li>
      <li><kbd>Swipe</kbd> to move on mobile devices</li>
    </ul>
  </section>

  <section id="topics" class="container">
    <h2>Topics</h2>
    <div class="row row-eq-height">
      <div class="col-md-6">
        <div class="panel panel-green">
          <div class="panel-heading">
            <h4 class="panel-title">Will Cover</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">What is ML</li>
            <li class="list-group-item">Job process</li>
            <li class="list-group-item">Python with scikit-learn</li>
            <li class="list-group-item">Major supervised algorithms</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6">
        <div class="panel panel-red">
          <div class="panel-heading">
            <h4 class="panel-title">Won't Cover</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Unsupervised and other algorithms</li>
            <li class="list-group-item">Data wrangling</li>
            <li class="list-group-item">Structured and sequential models</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes">

    </aside>
  </section>

</section>

<section>
  <section id="what-is-ml">
    <h1>What is machine learning?</h1>
    <img src="../img/ml.gif">
  </section>

  <section id="concepts" class="container">
    <h2>Core Concepts</h2>
    <div class="row row-eq-height">
      <div class="col-md-6">
        <div class="panel panel-orange">
          <div class="panel-heading">
            <h4 class="panel-title">Data</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Items</li>
            <li class="list-group-item">Features</li>
            <li class="list-group-item">Labels</li>
            <li class="list-group-item">Classes</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6">
        <div class="panel panel-purple">
          <div class="panel-heading">
            <h4 class="panel-title">Models</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Algorithms</li>
            <li class="list-group-item">Parameters</li>
            <li class="list-group-item">Hyper-parameters</li>
            <li class="list-group-item">Ensembles</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Like any discipline machine learning has some key concepts which you need to be familiar with.  We will explain these terms in more detail in the rest of the presentation.  Here's the basics:

      Machine learning works with **Data**, which is made up of individual **Items** or equivalently **Records**, **Points**, or **Samples**. **Features** are the various numerical and categorical traits which describe the item, and **Labels** are what you are attempting to predict based on the features.  **Classes** are the discrete possible values which labels may take in classification problems.

      **Models** are the tools you use to explain your samples.  In machine learning, models are built on, and generated by, **Algorithms**.  ML models can have radically different designs but all are intended to learn complex representations of arbitrary data.  The tunable parts of a model are called **Parameters**, and there are also **Hyperparameters** which control how the model is constructed and trained.  **Ensembles** are models which are built out of simpler models.
    </aside>
  </section>

  <section id="ml-flavors" class="container">
    <h2>Flavors of Machine Learning</h2>
    <div class="row row-eq-height">
      <div class="col-md-4">
        <div class="panel panel-red">
          <div class="panel-heading">
            <h4 class="panel-title">Supervised</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Classification</li>
            <li class="list-group-item">Regression</li>
            <li class="list-group-item">Ranking</li>
          </ul>
        </div>
      </div>
      <div class="col-md-4">
        <div class="panel panel-blue">
          <div class="panel-heading">
            <h4 class="panel-title">Unsupervised</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Clustering</li>
            <li class="list-group-item">Density estimation</li>
            <li class="list-group-item">Pattern identification</li>
            <li class="list-group-item">Representation learning</li>
          </ul>
        </div>
      </div>
      <div class="col-md-4">
        <div class="panel panel-green">
          <div class="panel-heading">
            <h4 class="panel-title">Hybrid</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Semi-supervised</li>
            <li class="list-group-item">Reinforcement learning</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There are four major types of machine learning problems:

      - Supervised: make predictions
        - Classification: predict from a sample of labels
        - Regression: predict a numeric value
        - Ranking: predict an order for data
      - Unsupervised: identify structure
        - Clustering: find groups within your data
        - Density estimation: identify distribution of samples in the feature domain
        - Pattern identification: identify common patterns
        - Representation learning: describe the data with much less information
      - Semi-supervised: supervised learning with additional unlabeled data to help characterize the feature domain
      - Reinforcement learning: data is generated by interaction with environment, attempting to maximize a reward
    </aside>
  </section>

  <section id="data-models">
    <h2>Data and models</h2>
    <ul>
      <li>Tabular data</li>
      <li>Structured data</li>
      <li>Graphical models</li>
      <li>Sequential or process models</li>
    </ul>

    <aside class="notes" data-markdown>
      The most common form of machine learning problem is to make a prediction for independent samples, where the features of the sample are not really related to each other.  But this is not always the case.  There are many others that require special formulation and considerations, such as:

      - [Structured data](NeuralNetworks.md#convolutional-layers): When there is a structural relationship between the features (such as pixels in an image)
      - [Graphical models](Probabilistic.md#graphical-techniques): Given samples for a collection of nodes, construct a network between the nodes
      - [Sequential or process models](NeuralNetworks.md#recurrent-neural-networks): When the input and/or output is a sequence of arbitrary length
    </aside>
  </section>

</section>

<section>
  <section id="steps">
    <h1>The steps of a machine learning job</h1>
    <ol>
      <li>Know your problem</li>
      <li>Know your data</li>
      <li>Set up your test harness</li>
      <li>Spot check a variety of algorithms</li>
      <li>Fine tune your best models</li>
    </ol>

    <aside class="notes" data-markdown>
      Machine learning isn't just algorithms.  There is a definite process to it, and applying the algorithms is in many ways the least complex part of it.  Let's take a look at them
    </aside>
  </section>

  <section id="problem">
    <h2>Know your problem</h2>
    <ol>
      <li>What problem are you trying to solve?</li>
      <li>Is machine learning the right choice?</li>
      <li>What kind of machine learning is warranted?</li>
      <li>Is there a simpler model which you should be using?</li>
    </ol>
    <blockquote>Machine learning is appropriate when the problem is too dynamic or complex to be handled by simple rules or models</blockquote>

    <aside class="notes" data-markdown>
      The most fundamental question for machine learning (or indeed most projects) is what problem are you trying to solve? Can machine learning help you with this, and what class of learning is required?

      Machine learning is most useful on tasks which can be solved but not simply explained.  For instance, you can easily describe the content of an image.  But can you describe the process you used to translate it from your eyes to your mouth?  You can't.  Others have tried with mediocre results.  Machine learning beat those algorithms handily.
    </aside>
  </section>

  <section id="data">
    <h2>Know your data</h2>
    <ul>
      <li>Where is it, how can I get it, and am I allowed to use it?</li>
      <li>What form is it in, and how do I convert it to a form that can be analyzed?</li>
      <li>What does it look like?</li>
      <li>Is it any good and do you have enough of it?</li>
    </ul>
    <blockquote>You need many more samples than features or parameters to learn effectively.</blockquote>

    <aside class="notes" data-markdown>
      You must also understand your data and how to use it.  Where is your data, how will you physically get it, and are you even allowed to use it? What form is it in, and how will you convert it into a form that can be analyzed?  Before you try applying machine learning, you should always try to look at your data directly first, to ensure that you aren't

      Most importantly, is your data any good and do you have enough of it to effectively learn from it?  Just like in statistics, bad data is worse than no data.  And just like in statistics, if you have more parameters than samples, you can't properly solve the problem.  You need many more samples than features or parameters to learn effectively.

      - Where is your data you need, how will you get to it, and are you allowed to use it?
        - How will you obtain it?
        - Do you have sufficient privileges?
        - Are you legally allowed to use your data to solve your problem? Be especially cautious about patient data.
      - What form is your data in, and how do I convert it to a form that can be analyzed?
        - How will you wrangle it into a form that can be analyzed?
      - What does your data look like?
        - Can you plot it and if so, is there a pattern?
        - What are the types of your features?
        - Will any of them need to be transformed into a different form for processing?
      - What data do you have to work with?
        - Do I have enough examples to train with?
        - Do you have many more samples than features? If not training may not be effective.
        - Do you have a lot of irrelevant features? These can introduce spurious correlations.
        - Are there any other features that you think might be useful and can you get them?
        - Is it labeled, and if so how?
        - Are your labels balanced?
        - Are any of your features exponential in nature? You may want to log transform them.
        - Are any of the features in your data identifiers? You may want to discard these: they can be used to overfit, and should not be interpreted as numbers.
    </aside>
  </section>

  <section id="harness">
    <h2>Set up your test harness</h2>
    <ul>
      <li>Set up your project directory and stick to your conventions. </li>
      <li>Clean up your data into a form suitable for analysis
        <ul>
          <li>Handle blanks with placeholders, imputation, or removal</li>
          <li>Convert categorical values to numeric (e.g. one-hot binary)</li>
          <li>Remove unhelpful features</li>
          <li>Add helpful ones if you can</li>
        </ul>
      </li>
      <li>Verify the scoring algorithm to use</li>
      <li>Determine if you need to compensate for biased input</li>
    </ul>

    <aside class="notes" data-markdown>

    </aside>
  </section>

  <section id="spot-check">
    <h2>Spot check a variety of algorithms</h2>
    <blockquote>I recommend setting up a reusable generator to yield a standard set of models to test</blockquote>
<pre><code class="py" data-trim data-noescape>
def standard_models():
  yield ExtraTreesClassifier(n_estimators=256)
  yield Pipeline([
    ('feat', RandomForestClassifier(max_depth=3,
      n_estimators=64, class_weight="auto")),
    ('et', ExtraTreesClassifier(n_estimators=256))])
  yield Pipeline([
    ('scale', StandardScaler()),
    ('svc', SVC())])
</code></pre>
  </section>

  <section id="fine-tune">
    <h2>Fine tune your best models</h2>
    <ul>
      <li>Try adding or removing features</li>
      <li>Use grid searches to fine-tune hyperparameters</li>
      <li>Make a weighted ensemble of multiple models</li>
    </ul>

<pre><code class="py" data-trim data-noescape>
from sklearn.grid_search import GridSearchCV, RandomizedSearchCV
</code></pre>
  </section>

</section>

<section>
  <section id="pitfalls">
    <h1>Problems and Pitfalls of Machine Learning</h1>

    <aside class="notes" data-markdown>
      There are a lot of things that can bog down a machine learning task.  Here are some of them.
    </aside>
  </section>

  <section id="no-free-lunch">
    <h2>No Free Lunch Theorem</h2>
    <p>No one algorithm will be suitable for all problems and any algorithm could be the right one.<span class="fragment" data-fragment-index="1">.. <i>but in practice only a handful of algorithms are really worth considering.</i></span></p>

    <div class="row row-eq-height fragment" data-fragment-index="1">
      <div class="col-md-6">
        <div class="well">
          <h4>Random Forests</h4>
        </div>
      </div>
      <div class="col-md-6">
        <div class="well">
          <h4>Neural Networks</h4>
        </div>
      </div>
    </div>
    <div class="row row-eq-height fragment" data-fragment-index="1">
      <div class="col-md-6">
        <div class="well">
          <h4>Support Vector Machines</h4>
        </div>
      </div>
      <div class="col-md-6">
        <div class="well">
          <h4>Cheap Algorithms</h4>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There is something called the no-free-lunch theorem, which basically says no one algorithm will be suitable for all problems and any algorithm could be the right one.  But I think some algorithms tend to be much better than others.

      And here they are: Random Forests and Neural Networks are definitely the big two.  Support Vector Machines are also good enough to be worth a shot, and there's no reason not to try various cheap algorithms.

      Let's discuss some of these algorithms in a bit more depth.  I may skip over these.
    </aside>
  </section>

  <section id="bias-variance" class="container">
    <div class="row">
      <div class="col-md-6">
        <svg id="bias-target" width="250" height="250">
          <g transform="translate(125, 125) scale(1.23)">
            <g class="target" stroke="black" stroke-width="0.75">
              <circle r="100" style="fill:white;"></circle>
              <circle r="90" style="fill:white;"></circle>
              <circle r="80" style="fill:gray;"></circle>
              <circle r="70" stroke="white" style="fill:gray;"></circle>
              <circle r="60" style="fill:lightblue;"></circle>
              <circle r="50" style="fill:lightblue;"></circle>
              <circle r="40" style="fill:lightcoral;"></circle>
              <circle r="30" style="fill:lightcoral;"></circle>
              <circle r="20" style="fill:yellow;"></circle>
              <circle r="10" style="fill:yellow;"></circle>
            </g>
            <g class="shots" fill="black" stroke="red" stroke-width="0.5"></g>
          </g>
        </svg>
      </div>
      <div class="col-md-6">
        <svg id="variance-target" width="250" height="250">
          <g transform="translate(125, 125) scale(1.23)">
            <g class="target" stroke="black" stroke-width="0.75">
              <circle r="100" style="fill:white;"></circle>
              <circle r="90" style="fill:white;"></circle>
              <circle r="80" style="fill:gray;"></circle>
              <circle r="70" stroke="white" style="fill:gray;"></circle>
              <circle r="60" style="fill:lightblue;"></circle>
              <circle r="50" style="fill:lightblue;"></circle>
              <circle r="40" style="fill:lightcoral;"></circle>
              <circle r="30" style="fill:lightcoral;"></circle>
              <circle r="20" style="fill:yellow;"></circle>
              <circle r="10" style="fill:yellow;"></circle>
            </g>
            <g class="shots" fill="black" stroke="red" stroke-width="0.5"></g>
          </g>
        </svg>
      </div>
    </div>
    <div style="display:none">
      <span class="fragment" id="first-shots"></span>
      <span class="fragment" id="many-shots"></span>
    </div>

    <div class="row">
      <div class="col-md-6">
        <h2>Bias</h2>
      </div>
      <div class="col-md-6">
        <h2>Variance</h2>
      </div>
    </div>

    <div class="row">
      <div class="col-md-6">
        <svg width="250" height="250">
          <g></g>
        </svg>
      </div>
      <div class="col-md-6">
        <svg width="250" height="250"></svg>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There are three sources of error in models **Bias** is the tendency of an model to favor specific representations of the data.  The counterpart of bias is **Variance**, the tendency of an algorithm to find very different representations for small variations in the data.  During training, high bias leads to **Underfitting** because the algorithm is incapable of finding or building a model, while high variance leads to **Overfitting** because it effectively memorizes the training data. What we really want is a model which **Generalizes** the data and can accurately and reliably handle whatever we throw at it.

      Let's use a metaphor. If you were shooting at a target, your bias is how far from the bullseye your shots are centered, and your variance is how spread apart your shots are.  A good model would have a tight grouping right in the center.

      The most important point about the bias-variance tradeoff is this: it is impossible to overcome intrinsic bias in a model, but it *is* possible to overcome variance.  We will discuss some tricks for this, but the most straightforward approach is simply more training with more data on complex, low-bias models. One more reason why "big data" is a big deal.

      Which brings us to the third source of error: insufficient or unrepresentative training data.  Garbage in, garbage out.
    </aside>
  </section>

  <section id="dimensionality">
    <h2>The curse of dimensionality</h2>
    <p class="small"><i>The more dimensions you have, the closer two random points in a bounded space are. </i></p>
    <img src="../img/dimensionality.png">
    <p class="small"><i>Consider reducing your features</i></p>
<pre><code class="py" data-trim data-noescape>
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
</code></pre>
    <aside class="notes">
      Irrelevant features are for all intents and purposes noise in your data.  This increases the risk of spurious correlations and makes most algorithms less efficient.  It also leads to higher memory consumption.

      There are deeper issues as well.  Without going into details, bounded high-dimensional spaces are spiky with most of the volume concentrated in the center.  In other words, the distance between random points [tends to be the same](http://yaroslavvb.com/papers/koppen-curse.pdf), and it becomes impossible to learn in many circumstances.  So the curse of dimensionality is really the curse of the central limit theorem.

      However, useful data is not random and generally has [lower true dimensionality](DimensionalityReduction.md) than the ones it is expressed in.  So real data does not necessarily suffer from this problem.  The moral of the story is, you should consider your features, particularly if your data has a large number of them.

      There are many ways to select smaller feature sets or perform dimensionality reduction.  If the identities of your features are not important than you may want to project into a lower dimensional space with PCA.  Otherwise, you probably want to estimate feature importances with a RandomForest and keep the important ones.
    </aside>
  </section>

  <section id="wrangling">
    <h2>Data Wrangling / Munging</h2>
    <img src="../img/wrangling.gif">
    <blockquote class="fragment">
      <p>You will spend 90% of your time just whipping your data into shape...</p>
      <p>I guarantee it.</p>
    </blockquote>
    <aside class="notes">
      Data wrangling (or munging or preparation) is widely considered to be the most tedious and time consuming part of data science.  You will spend 90% of your time just whipping your data into shape.  But as trivial as it sounds, you can't train a model without data.

        - May come in a wide variety of formats
        - May be messy and include missing data
        - May include unimportant data
        - May be too large for memory, requiring special techniques to process
    </aside>
  </section>

</section>

<section>
  <section id="environment">
    <h1>Environment</h1>
  </section>

  <section id="hardware-software" class="container">
    <h2>Hardware and Software</h2>
    <div class="row row-eq-height">
      <div class="col-md-6">
        <div class="panel panel-orange">
          <div class="panel-heading">
            <h4 class="panel-title">Machine</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Plenty of memory (8GB)</li>
            <li class="list-group-item">NVidia GPU, or decent CPU</li>
            <li class="list-group-item">Preferably Linux</li>
            <li class="list-group-item">Local or Server?</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6">
        <div class="panel panel-purple">
          <div class="panel-heading">
            <h4 class="panel-title">ML Platform</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item"><a href="https://www.r-project.org/">R</a></strong>: Premier statistical platform</li>
            <li class="list-group-item"><a href="https://www.python.org/">Python</a></strong>: Production ready, highly flexible</li>
            <li class="list-group-item"><em><a href="http://www.mathworks.com/products/matlab/">MatLab</a></em>: Proprietary numerical platform</li>
            <li class="list-group-item"><em><a href="http://julialang.org/">Julia</a></em>: New hotness</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Like any heavy computing task, in machine learning you must consider hardware and software.

      When choosing the system to perform machine learning (if you have a choice) you should take the following aspects into account:

      1. Plenty of memory, because both datasets and algorithms can require a lot of memory
      2. NVidia GPU, which can give you massive performance improvements for neural networks, often 30x.  It can make a huge difference in your ability to investigate.  Failing that, you want the best CPU you have.
      3. Most tools, libraries, and dependencies are available for Linux, OS X, and Windows.  But you will find that actually installing those on systems other than Linux can be quite painful, even on another Unix like OS X.  One time I had a compile fail because I didn't invoke the makefile from bash.  So if you can, save yourself some grief and use Linux.
      4. The first three requirements definitely favor a server, but there are plenty of benefits to being able to work without a connection.  If you can, do both, and keep things in sync with git, which you were already using RIGHT?

      Which brings us to your machine learning software platform.  Sure there are innumerable tools and services, but if you want to investigate properly you need a platform with a consistent interface which is kept up-to-date with the latest algorithms. At the moment, there are two primary ones and two secondary ones, all of which I have used at one time or another.

      [R](https://www.r-project.org/) is *the* premier open source statistical platform, and the most popular ML tool with the most algorithms.  It is widely used in academia, and so most new algorithms actually show up there first.

      [Python](https://www.python.org/) on the other hand is a general-purpose programming language.  Two packages are especially useful for machine Learning: [scikit-learn](http://scikit-learn.org) wraps many ML algorithms in a common API along with plenty of utilities, and  [Theano](http://deeplearning.net/software/theano) can be used to compile numerical algorithms to native code.  Significantly, python is production ready and excellent for data wrangling.  This is the environment I will be using today.

      [MatLab](http://www.mathworks.com/products/matlab/) is a numerical computing environment with extensive use in academic and research institutions. But it is also proprietary and expensive and doesn't see much use elsewhere.

      [Julia](http://julialang.org/) is a very new high-performance dynamic programming language. It is specifically designed for numerical and scientific computing but also aims to be an effective general purpose language.  The syntax is concise and the performance is actually quite good.  But it is very young and core APIs are still changing, so although its package ecosystem is growing rapidly I can't recommend it just yet.
    </aside>
  </section>

</section>

<section>

  <section id="sklearn">
    <h2>Example code with scikit-learn</h2>
  </section>

  <section id="train-test-predict">
    <h2 class="text-left">Standard train-test-predict workflow</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.ensemble import ExtraTreesClassifier

# Create a model
clf = ExtraTreesClassifier(n_estimators:1024,
  class_weight:"subsample")

# Fit the model
clf.fit(X_train, y_train)

# Verify performance
print clf.score(X_test, y_test)
>>> 0.82

# To make predictions
y_predict = clf.predict(X_predict)
</code></pre>

    <aside class="notes" data-markdown>
      So here is a typical example of training and testing models, and as you can see it is quite straightforward.  The nicest thing about scikit-learn is the consistency of their approach.  All methods, parameter names, and parameter values will have the same names to the greatest extent possible.  This means you can pick and choose your algorithms without having to write a lot of glue code.

      Here I am creating a new classifier with some designated parameters (really these are hyperparameters) using the constructor, and `fit` it to the features and labels.  Then I verify the performance on my validation set using `score`. If I am satisfied with the result, all I need to do to make predictions is call `predict`.
    </aside>
  </section>

  <section id="preprocess-pipeline">
    <h2 class="text-left">Preprocessing and Pipelines</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

# We could preprocess like this:
ss = StandardScaler()
X_train_prep = ss.fit_transform(X_train)
X_test_prep = ss.transform(X_test)
clf = SVC(kernel="rbf", class_weight="auto");
clf.fit(X_train_prep, y_train)

# Or we could simplify with a pipeline
clf = Pipeline([('ss', StandardScaler()), ("svc", SVC(kernel="rbf"))])

# You can even set parameters on it
clf.set_params(svc__class_weight="auto")

clf.fit(X_train, y_train)
</code></pre>

    <aside class="notes" data-markdown>
      If we are using a classifier which needs preprocessing (which is most of them) then there are a couple of ways to do that.

      You could do it the obvious way, fitting your preprocessor on the train data and then transforming both train and test to get the necessary data sets, then training your model.

      Or you could do it the easy way, and wrap your preprocessor (or preprocessors) and main classifier into a pipeline.  The resulting Pipeline objects behaves exactly like any other scikit-learn model.  You can even set parameters on it, because we named all of the parts of the pipeline.
    </aside>
  </section>

  <section id="splitting">
    <h2 class="text-left">Splitting data for validation</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.cross_validation import train_test_split,
  KFold, StratifiedShuffleSplit

# Quickly split into 80% train and 20% test
X_train, X_test, y_train, y_test =
  train_test_split(X, y, test_size=0.2)

# Or split into 5 folds
folds = KFold(y.length, 5, shuffle=True)
for i_train, i_test in folds:
  X_train, X_test = X[i_train], X[i_test]
  y_train, y_test = y[i_train], y[i_test]

# Or generate 5 random splits preserving class percentages
sss = StratifiedShuffleSplit(y, 5, test_size=0.2)

# etc.
</code></pre>
    <aside class="notes" data-markdown>
      With machine learning models the accuracy you get on the data you train with is usually much higher than what you will get for other data.  Which is why every machine learning workflow will require you to save some of your data for testing.

      The simplest way to do this is with the train_test_split function, which will randomly split all input matrices the same way.

      Of course one train-test split may by chance not produce representative results, so you usually want to split more than once.  There are many different strategies, but they all work the same: create a splitter and get back a series of tuples with the train and test indices,
    </aside>
  </section>

  <section id="pandas">
    <h2 class="text-left">Data i/o with Pandas</h2>
<pre><code class="py" data-trim data-noescape>
import pandas as pd

# Load the data from a csv file
df = pd.read_csv("input/data.csv");

# Clean up data

# Get features matrix 'X' for feature names (don't include ID!)
X = df[features].values

# Get labels vector 'y'
y = df["label_name"].values

# Write out data later...
predict_df = pd.DataFrame({'Id': ids, 'Label': predictions })
predict_df.to_csv("output/predict.csv")
</code></pre>
    <aside class="notes" data-markdown>
      Somewhere in the data wrangling process you will need to load your data for analysis (and maybe wrangle it some more).  And at the end of the job you will need to output it.  With tabular data, the pandas library is a great help.  This is essentially the dataframes object that is popular in R.  The full scope of its functionality is beyond what we have time to discuss, but know that it exists and that it makes working with tabular data easier.
    </aside>
  </section>

</section>

<section>
  <section id="tree-algorithms">
    <h1>Tree Algorithms</h1>
    <aside class="notes" data-markdown>
      There are many different algorithms for machine learning but they tend to fall into only a few major categories.  One of the most successful are the tree algorithms.  So let's take a look at those.
    </aside>
  </section>

  <section id="decision-tree" class="container">
    <div class="row">
      <div class="col-md-6">
        <h2 class="text-left">Decision Trees</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">Easily understood</li>
            <li class="list-group-item">No preprocessing</li>
            <li class="list-group-item">Fast and cheap</li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group">
            <li class="list-group-item">Overfits</li>
            <li class="list-group-item">No mathematical processing</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.tree import
  DecisionTreeClassifier, DecisionTreeRegressor
</code></pre>
    <aside class="notes" data-markdown>
      Decision trees are arguably the simplest and most intuitive machine learning model in existence.  They are essentially a flowchart: for each node starting at the root, you evaluate a rule and are directed to another node, until you reach a final decision. They require no preprocessing and are fast and cheap to train, store, and evaluate.

      How you actually build a given decision tree from the training data depends on your algorithm. With a simple algorithm and no constraints they can produce the best possible fit to the training data.

      Which is to say, they can easily overfit.  Also, they cannot perform any mathematical processing, so they do not generalize very well.
    </aside>
  </section>

  <section id="random-forest" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Random Forests</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">No preprocessing</li>
            <li class="list-group-item">Minimal tuning</li>
            <li class="list-group-item">Fast</li>
            <li class="list-group-item"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Reliably good results</a></li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group small">
            <li class="list-group-item">No mathematical processing</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.ensemble import
  RandomForestClassifier, RandomForestRegressor,
  ExtraTreesClassifier, ExtraTreesRegressor
</code></pre>
    <aside class="notes" data-markdown>
      A random forest is simply an ensemble of specially constructed decision trees.  Each tree is trained using a bootstraped dataset, and each decision node is based on a random subset of features.  Random forests have several really nice practical aspects:

      - Since they are based on decision trees, no preprocessing of features is necessary.
      - There are few parameters to tune and those parameters are very forgiving.  Essentially it's just the maximum depth, number of features to sample, and the number of trees. Interestingly, adding trees can only improve test accuracy.
      - Training and evaluation is embarassingly parallel, and typically quite fast.  You can add trees for accuracy or remove trees for performance at will.
      - Most importantly random forests work [very well on a very large number of problems](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf).

      In fact if I was really in a pinch and could use only one algorithm, it would be the extremely randomized trees variant, which is an even more random forest.  It will almost certainly give me an accurate result with no fuss whatsoever.

      Forests work well because [bagging is most effective on unstable algorithms that overfit easily](http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf)... and decision trees overfit more easily than just about anything.
    </aside>
  </section>

  <section id="decision-jungle" class="container" data-background="#ffff00" data-background-transition="zoom">
    <div class="row">
      <div class="col-md-4 nopadding">
        <svg width="300" height="600" style="background:white"></svg>
      </div>
      <div class="col-md-8">
        <h2>Bonus: Decision Jungles</h2>
        <div class="panel panel-default small">
          <ul class="list-group small">
            <li class="list-group-item">Use DAGs instead of trees</li>
            <li class="list-group-item">Less memory</li>
            <li class="list-group-item">Slightly more general</li>
            <li class="list-group-item">Microsoft... Kinect?</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Decision trees grow exponentially with depth, which can be a problem in constrained environments.  So microsoft has developed a variant of random forests called [decision jungles](http://research.microsoft.com/apps/pubs/?id=205439) which use directed acyclic graphs instead of trees.  So the DAG will grow only linearly past a certain point, and reduces redundancy.  They are more complicated to train but require less memory to store.  Also, they are also less prone to overfitting and generalize better.

      Fun fact: the invention of decision jungles at Microsoft was may have been prompted by the XBox Kinect controller, which famously uses random forests.
    </aside>
  </section>

</section>

<section>
  <section id="linear-algorithms">
    <h1>Linear Algorithms</h1>
    <aside class="notes" data-markdown>
      Trees work well but they have some inherent limitations.  And as you may have heard the hot algorithms for deep learning now are neural networks and related models.  These are all based on linear transformation.
    </aside>
  </section>

  <section id="linear-transformation" class="container">
    <div class="row">
      <div class="col-md-6">
        <h2 class="text-left">Linear layers</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">Mathematical processing</li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group">
            <li class="list-group-item">Tuning necessary</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6 nopadding">
        <p>$$\begin{align}
      s_j^{(1)} =& \sum_{i} x_{i} w^{(in\rightarrow 1)}_{i\rightarrow j}\\
      S^{(1)} =& X W^{(in\rightarrow 1)}\\
    \end{align}$$</p>
      </div>
    </div>

    <aside class="notes" data-markdown>

    </aside>
  </section>


  <section id="neural-network" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Neural Networks</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">Learn progressively higher-level features</li>
            <li class="list-group-item"></li>
            <li class="list-group-item"></li>
            <li class="list-group-item"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Reliably good results</a></li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group small">
            <li class="list-group-item">No mathematical processing</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.ensemble import
</code></pre>
    <aside class="notes" data-markdown>
    </aside>
  </section>

  <section id="extreme-learning-machines" data-background="#ffff00" data-background-transition="zoom">
    <h2>Extreme Learning Machines</h2>
    <svg width="800" height="300" style="background:white"></svg>
    <aside class="notes" data-markdown>
      Extreme Learning Machines are a variant of neural networks (usually with one relatively large hidden layer).  The input-hidden weights are initialized randomly (like usual).  The hidden-output weights are then calculated analytically (not like usual).  And that's it - no backpropagation necessary.  Amazingly, this comically simple and fast approach works pretty well.  In practice a few rounds of backpropagation will improve the results even more.

      Essentially, an extreme learning machine is just a logistic/linear regression, albeit on a random and non-linear transformation of the original features.  The fact that it works, and works well, demonstrates two important things: firstly, that discovering tractable features (through transformation or engineering) is *the* critical task of any machine learning pipeline; secondly, that a random linear transformation of arbitrary features can produce a new set of features tractable to linear analysis.

      > elm_kernel_m is the ELM with Gaussian kernel, tuning the regularization parameter and the kernel spread with values 2<sup>-5</sup>..2<sup>14</sup> and 2<sup>-16</sup>..2<sup>8</sup> respectively.  Neurons between 3 and 200. From Huang et al.
    </aside>
  </section>

</section>

<section>

  <section id="other-algorithms">
    <h1>Other Algorithms</h1>
    <aside class="notes" data-markdown>
      Although random forests and neural networks are the big performers these days, there are still some other algorithms worth trying.  Remember, the no-free-lunch theorem says we should at least look at other algorithms before pronouncing a winner.  And some of these algorithms work well enough or are cheap enough to give them a look.
    </aside>
  </section>

  <section id="svm" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <img src="../img/svc.gif" style="width:320px;height:auto">
        <img src="../img/svc-projection.gif" style="width:320px;height:auto;clip:rect(0px,140px,140px,0px);">
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Support Vector Machines</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">High quality class boundaries</li>
            <li class="list-group-item"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Pretty good accuracy</a></li>
            <li class="list-group-item">One-class training!</li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group small">
            <li class="list-group-item">Lots of tuning</li>
            <li class="list-group-item">Overfit easily</li>
            <li class="list-group-item">Not suitable for large training sets</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.svm import SVC, SVR
</code></pre>
    <aside class="notes" data-markdown>
      Support Vector Machines exploit something known as the "kernel trick", which is a way of projecting your samples into a higher dimensional combinatorial space.  The trick is that by using certain mathematically behaved functions that work on the dot products between the sample vectors, we don't actually have to *make* the projection to solve the SVM.  The final model after training is based on a selection of the training samples, the so-called support vectors.  Presuming you don't use too many the function is quick to evaluate.

      Support vectors generate very high quality class boundaries and can give good accuracy in many cases.  They also can be trained on a single class for outlier detection.

      But there are big catches.  Tuning the parameters can be quite tricky, as SVMs overfit easily.  And most significantly, the memory requirements are quadratic, which can be a huge burden if you aren't careful.
    </aside>
  </section>

</section>

<section>
  <section id="end" class="container">
    <h2>Thank you!</h2>
    <br>

    <div class="row">
      <div class="col-md-12 text-left">
        <h4>Further reading</h4>
        <ul>
          <li><a href="https://htmlpreview.github.io/?https://raw.githubusercontent.com/jmgore75/PracticalML/master/presentation/conference2015.html">Replay</a> this presentation on <a href="https://github.com/jmgore75/PracticalML/">GitHub</a></li>
          <li><a href="http://machinelearningmastery.com/">Machine learning mastery</a> blog</li>
          <li><a href="https://www.udacity.com/course/machine-learning--ud262">Machine Learning</a> Udacity course series</li>
        </ul>
      </div>
    <div>
  </section>

  <section id="tools">
    <div class="row">
      <div class="col-md-6 text-left">
        <h4>Development Tools</h4>
        <ul>
          <li><a href="https://www.python.org/">Python</a> language</li>
          <li><a href="http://scikit-learn.org">scikit-learn</a> ML library</li>
        </ul>
      </div>
      <div class="col-md-6 text-left">
        <h4>Presentation Tools</h4>
        <ul>
          <li><a href="https://github.com/hakimel/reveal.js">Reveal.js</a> presentation framework</li>
          <li><a href="http://d3js.org/">d3.js</a> charting library</li>
        </ul>
      </div>
    </div>

    <aside class="notes">

    </aside>
  </section>

</section>

</div>
</div>

<!-- Setup -->

<script src="../reveal.js/lib/js/head.min.js"></script>
<script src="../reveal.js/js/reveal.js"></script>

<script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,

    transition: 'concave',

    // Optional reveal.js plugins
    dependencies: [
      { src: '../reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: '../reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: '../reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: '../reveal.js/plugin/math/math.js'},
      { src: '../reveal.js/plugin/highlight/highlight.js', async: true, condition: function() {
          return !!document.querySelector( 'code' );
        }, callback: function() {
          hljs.initHighlightingOnLoad();
        }
      },
      { src: '../reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: '../reveal.js/plugin/remotes/remotes.js', async: true },
      { src: '../reveal.js/plugin/notes/notes.js', async: true }
    ]
  });
</script>

<script src="../js/d3.min.js"></script>
<script src="../js/dt.js"></script>
<script>
  //Animations
  (function () {
    var bias_target = new Target(d3.select("#bias-target g.shots"));
    bias_target.shooter = new Shooter(50, Math.PI*1.3, 10);

    var variance_target = new Target(d3.select("#variance-target g.shots"));
    variance_target.shooter = new Shooter(10, Math.PI*0.32, 30);

    Target.prototype.placeShots = function (shot_ids, duration) {
      var shotMap = this.shooter.shots;
      var shots = [];
      shot_ids.forEach(function (shot_id) {
        if (shotMap[shot_id]) {
          shots = shots.concat(shotMap[shot_id]);
        }
      })
      this.doShots(shots, duration);
    }

    Target.prototype.showShots = function () {
      this.doShots(this.shooter.shots["first-shots"]);
    }

    function handleChange(event) {
      var duration = event ? 1000 : 0;
      var cslide = Reveal.getCurrentSlide();
      if (cslide.id === "bias-variance") {
        var vfrag = cslide.querySelectorAll(".fragment.visible");
        var shots = [];
        for (var i = 0; i < vfrag.length; i++) {
          if (vfrag[i].id) {
            shots.push(vfrag[i].id);
          }
        }
        bias_target.placeShots(shots, duration);
        variance_target.placeShots(shots, duration);
      }
    }

    function handleSlide(event) {
      if (event.previousSlide && event.previousSlide.id === "bias-variance") {
        bias_target.showShots();
        variance_target.showShots();
      }
      if (event.currentSlide && event.currentSlide.id === "bias-variance") {
        handleChange();
      }
    }

    Reveal.addEventListener( 'fragmentshown', handleChange);

    Reveal.addEventListener( 'fragmenthidden', handleChange);

    Reveal.addEventListener('ready', handleSlide);

    Reveal.addEventListener( 'slidechanged', handleSlide);

    var graphMap = {
      "title" : [new Tree(5, 3, 0.1), new DAG(7, 3, 8), new NN(4, 8, 8, 8, 3)],
      "decision-tree" : [new Tree(5, 3, 0.2), new Tree(4, 3, 0), new Tree(4, 2, 0.2)],
      "random-forest" : [new Forest(4, 3, 0.1), new Forest(4, 2, 0.2), new Forest(5, 3, 0.2)],
      "decision-jungle" : [new DAG(7, 3, 8), new DAG(8, 2, 6), new DAG(7, 3, 8)],
      "neural-network"  : [new NN(4, 8, 8, 8, 3)],
      "extreme-learning-machines" : [new NN(4, 32, 3)]
    }

    function loopGraphs(slide, graphs) {
      if (!slide) {
        return;
      }
      var graphs = Array.prototype.slice.call(arguments);
      var slide = graphs.shift();

      if (!graphs.length) {
        graphs = graphMap[slide.id] || [];
      }
      var svg = d3.select(slide).select("svg");

      if (svg[0][0] && graphs.length) {
        var t = svg.interrupt().transition();
        svg.selectAll("*").remove();

        function loop(t) {
          graphs.forEach(function (graph) {
            //Update each on a timer
            graph.steps.forEach(function (step) {
              t = t.transition().duration(step.duration || 0).each("start", function () {
                displayStep(step, svg);
              });
            });
            t = t.transition().duration(3000);
            t = t.transition().duration(clearStep.duration || 0).each("start", function () {
              displayStep(clearStep, svg);
            });
            t = t.transition().duration(500).each("start", function () {
              svg.selectAll("*").remove();
            });
          })

          t = t.each("end", function () {
            loop(t);
          });
        }

        loop(t);
      }
    }

    function showGraph(slide, graph) {
      if (!slide) {
        return;
      }
      if (!graph) {
        var graphs = graphMap[slide.id];
        if (graphs) {
          graph = graphs[0];
        }
      }
      var svg = d3.select(slide).select("svg");
      if (svg[0][0] && graph) {
        svg.interrupt().transition();
        svg.selectAll("*").remove();
        var step = graph.lastStep();
        if (step) {
          displayStep(step, svg, 0);
        }
      }
    }

    Reveal.addEventListener('ready', function (event) {
      for (var id in graphMap) {
        var slide = document.getElementById(id);
        if (slide) {
          showGraph(slide)
        }
      }
      loopGraphs(event.currentSlide);
    });

    Reveal.addEventListener( 'slidechanged', function(event) {
      showGraph(event.previousSlide);
      loopGraphs(event.currentSlide);
    } );

  })();

</script>

</body>
</html>
