<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>A Practical Guide to Applied Machine Learning</title>

  <meta name="description" content="Using python and scikit-learn, we'll walk through a machine learning process from start to finish discussing when to apply machine learning, the most successful algorithms, best practices, and even a little philosophy.">
  <meta name="author" content="Jeremy Gore">

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <link rel="stylesheet" href="../reveal.js/css/reveal.css">
  <link rel="stylesheet" href="../css/tessella.css" id="theme">
  <link rel="stylesheet" href="../css/presentation.css">

  <!-- Code syntax highlighting -->
  <link rel="stylesheet" href="../reveal.js/lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement('link');
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match(/print-pdf/gi) ? '../reveal.js/css/print/pdf.css' : '../reveal.js/css/print/paper.css';
    document.getElementsByTagName('head')[0].appendChild(link);
  </script>

<!--[if lt IE 9]>
<script src="lib/js/html5shiv.js"></script>
<![endif]-->
</head>

<body>
<div class="reveal">
<div class="slides">

<section>
  <section id="title" class="container">
    <div class="col-md-4 nopadding">
      <svg width="300" height="600"></svg>
    </div>
    <div class="col-md-8 text-right">
      <img class="noborder" style="width:300px;height:auto; margin-bottom:1em; " src='../img/Tessella_Logo.png'>
      <h2 class="text-right">A Practical Guide to Applied Machine Learning</h2>
      <h2 class="text-right"><small>Jeremy Gore<br>Autumn Conference 2015-10-05</small></h2>
    </div>

    <aside class="notes" data-markdown>
      To say machine learning is a big subject is a big understatement, but it may be easier than you think.

      Using python and scikit-learn, we'll walk through a machine learning process from start to finish discussing when to apply machine learning, the most successful algorithms, best practices, and even a little philosophy.
    </aside>
  </section>

  <section id="instructions" data-background="#996699" data-background-transition="zoom">
    <h3>Presentation instructions</h3>
    <ul>
      <li>Press <kbd>ESC</kbd> to enter the slide overview</li>
      <li>Press <kbd>F</kbd> to display full screen</li>
      <li>Press <kbd>B</kbd> or <kbd>.</kbd> to pause the presentation</li>
      <li>Press <kbd>S</kbd> to display the speaker view with notes</li>
      <li><kbd>Alt+click</kbd> on any element to zoom in on it, <kbd>Alt+click</kbd> anywhere to zoom back out</li>
      <li><kbd>Swipe</kbd> to move on mobile devices</li>
    </ul>
  </section>

</section>

<section id="topics" class="container">
  <h2>Topics</h2>
  <div class="row row-eq-height">
    <div class="col-md-6">
      <div class="panel panel-green">
        <div class="panel-heading">
          <h4 class="panel-title">Will Cover</h4>
        </div>
        <ul class="list-group">
          <li class="list-group-item">What is ML</li>
          <li class="list-group-item">Job process</li>
          <li class="list-group-item">Python with scikit-learn</li>
          <li class="list-group-item">Major supervised algorithms</li>
        </ul>
      </div>
    </div>
    <div class="col-md-6">
      <div class="panel panel-red">
        <div class="panel-heading">
          <h4 class="panel-title">Won't Cover</h4>
        </div>
        <ul class="list-group">
          <li class="list-group-item">Other algorithms</li>
          <li class="list-group-item">Other Plaforms</li>
          <li class="list-group-item">Structured and sequential models</li>
        </ul>
      </div>
    </div>
  </div>

  <aside class="notes">
    Here's a brief overview of what we'll be covering today.  We'll review what machine learning is and what it's all about, before reviewing the job process with Python and scikit-learn.  Along the way we'll discuss some major supervised algorithms.

    We *won't* be covering other algorithms, platforms, or more complex data models, but rest assured much of what we will discuss today can be applied directly in these other contexts.  
  </aside>
</section>

<section>
  <section id="what-is-ml">
    <h1>What is machine learning?</h1>
    <img src="../img/ml.gif">
  </section>

  <section id="concepts" class="container">
    <h2>Core Concepts</h2>
    <div class="row row-eq-height">
      <div class="col-md-6">
        <div class="panel panel-orange">
          <div class="panel-heading">
            <h4 class="panel-title">Data</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Items</li>
            <li class="list-group-item">Features</li>
            <li class="list-group-item">Labels</li>
            <li class="list-group-item">Classes</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6">
        <div class="panel panel-purple">
          <div class="panel-heading">
            <h4 class="panel-title">Models</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Algorithms</li>
            <li class="list-group-item">Parameters</li>
            <li class="list-group-item">Hyper-parameters</li>
            <li class="list-group-item">Ensembles</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Like any discipline machine learning has some key concepts which you need to be familiar with.  We will explain these terms in more detail in the rest of the presentation.  Here's the basics:

      Machine learning works with **Data**, which is made up of individual **Items** or equivalently **Records**, **Points**, or **Samples**. **Features** are the various numerical and categorical traits which describe the item, and **Labels** are what you are attempting to predict based on the features.  **Classes** are the discrete possible values which labels may take in classification problems.

      **Models** are the tools you use to explain your samples.  In machine learning, models are built on, and generated by, **Algorithms**.  ML models can have radically different designs but all are intended to learn complex representations of arbitrary data.  The tunable parts of a model are called **Parameters**, and there are also **Hyperparameters** which control how the model is constructed and trained.  **Ensembles** are models which are built out of simpler models.
    </aside>
  </section>

  <section id="hardware-software" class="container">
    <h2>Hardware and Software</h2>
    <div class="row row-eq-height">
      <div class="col-md-6">
        <div class="panel panel-orange">
          <div class="panel-heading">
            <h4 class="panel-title">Machine</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Plenty of memory (8GB)</li>
            <li class="list-group-item">NVidia GPU, or decent CPU</li>
            <li class="list-group-item">Preferably Linux</li>
            <li class="list-group-item">Local or Server?</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6">
        <div class="panel panel-purple">
          <div class="panel-heading">
            <h4 class="panel-title">ML Platform</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item"><a href="https://www.r-project.org/">R</a></strong>: Premier statistical platform</li>
            <li class="list-group-item"><a href="https://www.python.org/">Python</a></strong>: Production ready, highly flexible</li>
            <li class="list-group-item"><em><a href="http://www.mathworks.com/products/matlab/">MatLab</a></em>: Proprietary numerical platform</li>
            <li class="list-group-item"><em><a href="http://julialang.org/">Julia</a></em>: New hotness</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Like any heavy computing task, in machine learning you must consider hardware and software.

      When choosing the system to perform machine learning (if you have a choice) you should take the following aspects into account:

      1. Plenty of memory, because both datasets and algorithms can require a lot of memory
      2. NVidia GPU, which can give you massive performance improvements for neural networks, often 30x.  It can make a huge difference in your ability to investigate.  Failing that, you want the best CPU you have.
      3. Most tools, libraries, and dependencies are available for Linux, OS X, and Windows.  But you will find that actually installing those on systems other than Linux can be quite painful, even on another Unix like OS X.  One time I had a compile fail because I didn't invoke the makefile from bash.  So if you can, save yourself some grief and use Linux.
      4. The first three requirements definitely favor a server, but there are plenty of benefits to being able to work without a connection.  If you can, do both, and keep things in sync with git, which you were already using RIGHT?

      Which brings us to your machine learning software platform.  Sure there are innumerable tools and services, but if you want to investigate properly you need a platform with a consistent interface which is kept up-to-date with the latest algorithms. At the moment, there are two primary ones and two secondary ones, all of which I have used at one time or another.

      [R](https://www.r-project.org/) is *the* premier open source statistical platform, and the most popular ML tool with the most algorithms.  It is widely used in academia, and so most new algorithms actually show up there first.

      [Python](https://www.python.org/) on the other hand is a general-purpose programming language.  Two packages are especially useful for machine Learning: [scikit-learn](http://scikit-learn.org) wraps many ML algorithms in a common API along with plenty of utilities, and  [Theano](http://deeplearning.net/software/theano) can be used to compile numerical algorithms to native code.  Significantly, python is production ready and excellent for data wrangling.  This is the environment I will be using today.

      [MatLab](http://www.mathworks.com/products/matlab/) is a numerical computing environment with extensive use in academic and research institutions. But it is also proprietary and expensive and doesn't see much use elsewhere.

      [Julia](http://julialang.org/) is a very new high-performance dynamic programming language. It is specifically designed for numerical and scientific computing but also aims to be an effective general purpose language.  The syntax is concise and the performance is actually quite good.  But it is very young and core APIs are still changing, so although its package ecosystem is growing rapidly I can't recommend it just yet.
    </aside>
  </section>

  <section id="py-ml-stack" class="container">
    <h2>Recommended Python ML Stack</h2>
    <div class="row">
      <div class="col-md-4">
        <ol class="list-group small">
          <li class="list-group-item">Python</li>
          <li class="list-group-item">Pandas</li>
          <li class="list-group-item">scikit-learn</li>
          <li class="list-group-item">scikit-neuralnetwork</li>
          <li class="list-group-item">Theano</li>
        </ol>
      </div>
      <div class="col-md-8">
        <img class="noborder" width="400" height="auto" style="width:400;height:auto" src="../img/logo-anaconda.svg">
        <p><a href="https://www.continuum.io/">https://www.continuum.io/</a></p>
        <blockquote class="small">You must install a compiler (g++) and ideally NVIDIA's CUDA</blockquote>
<pre><code class="sh" data-trim>
$ conda install mingw libpython
$ pip install theano scikit-neuralnetwork
</code></pre>
      </div>
    </div>

    <aside class="notes" data-markdown>
      To handle the code I am going to show you today you will need to install the following software:

      - Python
      - Pandas for data io
      - scikit-learn for most machine learning algorithms
      - and scikit-neuralnetwork for neural networks, using Theano

      You could install those individually, but I would recommend that you use the Anaconda distribution for scientific python, and then install theano and scikit-neuralnetwork using pip.

      But, theano is actually a python library for just-in-time compilation of numerical algorithms.  It requires a compiler.  Ideally, if you have a compatible NVIDIA GPU you can install their CUDA tools for even faster execution.  It is well worth it but installation can be a bit of a pain.
    </aside>
  </section>

</section>

<section>
  <section id="steps">
    <h1>The steps of a machine learning job</h1>
    <ol>
      <li>Know your problem</li>
      <li>Know your data</li>
      <li>Set up your test harness</li>
      <li>Spot check a variety of algorithms</li>
      <li>Fine tune your best models</li>
    </ol>

    <aside class="notes" data-markdown>
      Machine learning isn't just algorithms.  There is a definite process to it, and applying the algorithms is in many ways the least complex part of it.  Let's take a look at them
    </aside>
  </section>

</section>

<section>
  <section id="problem">
    <h1 class="text-left">1. Know your problem</h1>
    <ul>
      <li>What problem are you trying to solve?</li>
      <li>Is machine learning the right choice?</li>
      <li>What kind of machine learning is warranted?</li>
      <li>Is there a simpler model which you should be using?</li>
      <li>What scoring algorithm should you use?</li>
    </ul>
    <blockquote>Machine learning is appropriate when the problem is too dynamic or complex to be handled by simple rules or models</blockquote>

    <aside class="notes" data-markdown>
      The most fundamental question for machine learning (or indeed most projects) is what problem are you trying to solve? Can machine learning help you with this, and what class of learning is required?

      Machine learning is most useful on tasks which can be solved but not simply explained.  For instance, you can easily describe the content of an image.  But can you describe the process you used to translate it from your eyes to your mouth?  You can't.  Others have tried with mediocre results.  Machine learning beat those algorithms handily.
    </aside>
  </section>

  <section id="ml-flavors" class="container">
    <h2>Flavors of Machine Learning</h2>
    <div class="row row-eq-height">
      <div class="col-md-4">
        <div class="panel panel-red">
          <div class="panel-heading">
            <h4 class="panel-title">Supervised</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Classification</li>
            <li class="list-group-item">Regression</li>
            <li class="list-group-item">Ranking</li>
          </ul>
        </div>
      </div>
      <div class="col-md-4">
        <div class="panel panel-blue">
          <div class="panel-heading">
            <h4 class="panel-title">Unsupervised</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Clustering</li>
            <li class="list-group-item">Density estimation</li>
            <li class="list-group-item">Pattern identification</li>
            <li class="list-group-item">Representation learning</li>
          </ul>
        </div>
      </div>
      <div class="col-md-4">
        <div class="panel panel-green">
          <div class="panel-heading">
            <h4 class="panel-title">Hybrid</h4>
          </div>
          <ul class="list-group">
            <li class="list-group-item">Semi-supervised</li>
            <li class="list-group-item">Reinforcement learning</li>
          </ul>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There are four major types of machine learning problems:

      - Supervised: make predictions
        - Classification: predict from a sample of labels
        - Regression: predict a numeric value
        - Ranking: predict an order for data
      - Unsupervised: identify structure
        - Clustering: find groups within your data
        - Density estimation: identify distribution of samples in the feature domain
        - Pattern identification: identify common patterns
        - Representation learning: describe the data with much less information
      - Semi-supervised: supervised learning with additional unlabeled data to help characterize the feature domain
      - Reinforcement learning: data is generated by interaction with environment, attempting to maximize a reward
    </aside>
  </section>

  <section id="data-models">
    <h2>Data and models</h2>
    <ul>
      <li>Tabular data</li>
      <li>Structured data</li>
      <li>Graphical models</li>
      <li>Sequential or process models</li>
    </ul>

    <aside class="notes" data-markdown>
      The most common form of machine learning problem is to make a prediction for independent samples, where the features of the sample are not really related to each other.  But this is not always the case.  There are many others that require special formulation and considerations, such as:

      - [Structured data](NeuralNetworks.md#convolutional-layers): When there is a structural relationship between the features (such as pixels in an image)
      - [Graphical models](Probabilistic.md#graphical-techniques): Given samples for a collection of nodes, construct a network between the nodes
      - [Sequential or process models](NeuralNetworks.md#recurrent-neural-networks): When the input and/or output is a sequence of arbitrary length
    </aside>
  </section>

</section>

<section>
  <section id="data">
    <h1 class="text-left">2. Know your data</h1>
    <ul class="small">
      <li>Where is it, how can I get it, and am I allowed to use it?</li>
      <li>What form is it in, and how do I convert it to a form that can be analyzed?</li>
      <li>What does it look like?</li>
      <li>Is it any good and do you have enough of it?</li>
      <li>Are your classes biased and do you need to compensate for that? </li>
    </ul>
    <blockquote>You need many more samples than features or parameters to learn effectively.</blockquote>

    <aside class="notes" data-markdown>
      You must also understand your data and how to use it.  Where is your data, how will you physically get it, and are you even allowed to use it? What form is it in, and how will you convert it into a form that can be analyzed?  Before you try applying machine learning, you should always try to look at your data directly first, to ensure that you aren't

      Most importantly, is your data any good and do you have enough of it to effectively learn from it?  Just like in statistics, bad data is worse than no data.  And just like in statistics, if you have more parameters than samples, you can't properly solve the problem.  You need many more samples than features or parameters to learn effectively.

      - Where is your data you need, how will you get to it, and are you allowed to use it?
        - How will you obtain it?
        - Do you have sufficient privileges?
        - Are you legally allowed to use your data to solve your problem? Be especially cautious about patient data.
      - What form is your data in, and how do I convert it to a form that can be analyzed?
        - How will you wrangle it into a form that can be analyzed?
      - What does your data look like?
        - Can you plot it and if so, is there a pattern?
        - What are the types of your features?
        - Will any of them need to be transformed into a different form for processing?
      - What data do you have to work with?
        - Do I have enough examples to train with?
        - Do you have many more samples than features? If not training may not be effective.
        - Do you have a lot of irrelevant features? These can introduce spurious correlations.
        - Are there any other features that you think might be useful and can you get them?
        - Is it labeled, and if so how?
        - Are your labels balanced?
        - Are any of your features exponential in nature? You may want to log transform them.
        - Are any of the features in your data identifiers? You may want to discard these: they can be used to overfit, and should not be interpreted as numbers.
      - Are your classes biased?
        - You may want to use an algorithm that can compensate for that bias.
    </aside>
  </section>

  <section id="wrangling" class="container">
    <h2>Data Wrangling / Munging</h2>
    <div class="row">
      <div class="col-md-6">
        <img src="../img/wrangling.gif">
        <blockquote class="small">Good luck!</blockquote>
      </div>
      <div class="col-md-6">
        <div class="panel panel-red">
          <ul class="list-group small">
            <li class="list-group-item">Convert to correct format</li>
            <li class="list-group-item">Handle blanks</li>
            <li class="list-group-item">Convert categorical values to numeric</li>
            <li class="list-group-item">Remove unhelpful features</li>
            <li class="list-group-item">Add helpful features</li>
          </ul>
        </div>
      </div>
    <aside class="notes">
      Data wrangling (or munging or preparation) is widely considered to be the most tedious and time consuming part of data science.  You will spend 90% of your time just whipping your data into shape.  But as trivial as it sounds, you can't train a model without data.

        - May come in a wide variety of formats
        - May be messy and include missing data
        - May include unimportant data
        - May be too large for memory, requiring special techniques to process
    </aside>
  </section>

  <section id="pandas">
    <h2 class="text-left">Data i/o with Pandas</h2>
<pre><code class="py" data-trim data-noescape>
import pandas as pd

# Load the data from a csv file
df = pd.read_csv("input/data.csv");

# Clean up data

# Get features matrix 'X' for feature names (don't include ID!)
X = df[features].values

# Get labels vector 'y'
y = df["label_name"].values

# Write out data later...
predict_df = pd.DataFrame({'Id': ids, 'Label': predictions })
predict_df.to_csv("output/predict.csv")
</code></pre>
    <aside class="notes" data-markdown>
      Somewhere in the data wrangling process you will need to load your data for analysis (and maybe wrangle it some more).  And at the end of the job you will need to output it.  With tabular data, the pandas library is a great help.  This is essentially the dataframes object that is popular in R.  The full scope of its functionality is beyond what we have time to discuss, but know that it exists and that it makes working with tabular data easier.
    </aside>
  </section>

  <section id="dimensionality">
    <h2>The curse of dimensionality</h2>
    <p class="small"><i>The more dimensions you have, the closer two random points in a bounded space are. </i></p>
    <img src="../img/dimensionality.png">
    <p class="small"><i>Consider reducing your features</i></p>
<pre><code class="py" data-trim data-noescape>
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
</code></pre>
    <aside class="notes">
      Irrelevant features are for all intents and purposes noise in your data.  This increases the risk of spurious correlations and makes most algorithms less efficient.  It also leads to higher memory consumption.

      There are deeper issues as well.  Without going into details, bounded high-dimensional spaces are spiky with most of the volume concentrated in the center.  In other words, the distance between random points [tends to be the same](http://yaroslavvb.com/papers/koppen-curse.pdf), and it becomes impossible to learn in many circumstances.  So the curse of dimensionality is really the curse of the central limit theorem.

      However, useful data is not random and generally has [lower true dimensionality](DimensionalityReduction.md) than the ones it is expressed in.  So real data does not necessarily suffer from this problem.  The moral of the story is, you should consider your features, particularly if your data has a large number of them.

      There are many ways to select smaller feature sets or perform dimensionality reduction.  If the identities of your features are not important than you may want to project into a lower dimensional space with PCA.  Otherwise, you probably want to estimate feature importances with a RandomForest and keep the important ones.
    </aside>
  </section>

</section>

<section>
  <section id="harness">
    <h1 class="text-left">3. Set up your test harness</h1>
    <ul>
      <li>Set up your project directory</li>
      <li>Clean up your data into a form suitable for analysis</li>
      <li>Verify the scoring algorithm to use</li>
      <li>Set up your analysis routines</li>
    </ul>

    <aside class="notes" data-markdown>
      Now that you understand the problem and your data, it's time to prepare to make predictions.  First set up your work directory, then get your data into a form suitable for loading.  That means cleaning it up, transforming features if necessary, and handling blanks.  Remove unhelpful features, as these will only slow you down.  Then verify your score
    </aside>
  </section>

  <section id="train-test-predict">
    <h2 class="text-left">Standard train-test-predict workflow</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.ensemble import ExtraTreesClassifier

# Create a model
clf = ExtraTreesClassifier(n_estimators:1024,
  class_weight:"subsample")

# Fit the model
clf.fit(X_train, y_train)

# Verify performance
print clf.score(X_test, y_test)
>>> 0.82

# To make predictions
y_predict = clf.predict(X_predict)
</code></pre>

    <aside class="notes" data-markdown>
      So here is a typical example of training and testing models, and as you can see it is quite straightforward.  The nicest thing about scikit-learn is the consistency of their approach.  All methods, parameter names, and parameter values will have the same names to the greatest extent possible.  This means you can pick and choose your algorithms without having to write a lot of glue code.

      Here I am creating a new classifier with some designated parameters (really these are hyperparameters) using the constructor, and `fit` it to the features and labels.  Then I verify the performance on my validation set using `score`. If I am satisfied with the result, all I need to do to make predictions is call `predict`.
    </aside>
  </section>

  <section id="preprocess-pipeline">
    <h2 class="text-left">Preprocessing and Pipelines</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

# We could preprocess like this:
ss = StandardScaler()
X_train_prep = ss.fit_transform(X_train)
X_test_prep = ss.transform(X_test)
clf = SVC(kernel="rbf", class_weight="auto");
clf.fit(X_train_prep, y_train)

# Or we could simplify with a pipeline
clf = Pipeline([('ss', StandardScaler()), ("svc", SVC(kernel="rbf"))])

# You can even set parameters on it
clf.set_params(svc__class_weight="auto")

clf.fit(X_train, y_train)
</code></pre>

    <aside class="notes" data-markdown>
      If we are using a classifier which needs preprocessing (which is most of them) then there are a couple of ways to do that.

      You could do it the obvious way, fitting your preprocessor on the train data and then transforming both train and test to get the necessary data sets, then training your model.

      Or you could do it the easy way, and wrap your preprocessor (or preprocessors) and main classifier into a pipeline.  The resulting Pipeline objects behaves exactly like any other scikit-learn model.  You can even set parameters on it, because we named all of the parts of the pipeline.
    </aside>
  </section>

  <section id="splitting">
    <h2 class="text-left">Splitting data for validation</h2>
<pre><code class="py" data-trim data-noescape>
from sklearn.cross_validation import train_test_split,
  KFold, StratifiedShuffleSplit

# Quickly split into 80% train and 20% test
X_train, X_test, y_train, y_test =
  train_test_split(X, y, test_size=0.2)

# Or split into 5 folds
folds = KFold(y.length, 5, shuffle=True)
for i_train, i_test in folds:
  X_train, X_test = X[i_train], X[i_test]
  y_train, y_test = y[i_train], y[i_test]

# Or generate 5 random splits preserving class percentages
sss = StratifiedShuffleSplit(y, 5, test_size=0.2)

# etc.
</code></pre>
    <aside class="notes" data-markdown>
      With machine learning models the accuracy you get on the data you train with is usually much higher than what you will get for other data.  Which is why every machine learning workflow will require you to save some of your data for testing.

      The simplest way to do this is with the train_test_split function, which will randomly split all input matrices the same way.

      Of course one train-test split may by chance not produce representative results, so you usually want to split more than once.  There are many different strategies, but they all work the same: create a splitter and get back a series of tuples with the train and test indices,
    </aside>
  </section>

</section>

<section>
  <section id="spot-check">
    <h1 class="text-left">4. Spot check a variety of algorithms</h1>
    <img class="noborder" src="../img/algorithms.png">
    <aside class="notes">
      Once you have your harness set up, it's time to actually test some algorithms.  You should check a range of algorithms to identify which ones are performing well for you.  Be sure to keep track of what you have checked so far.
    </aside>
  </section>

  <section id="no-free-lunch">
    <h2>No Free Lunch Theorem</h2>
    <p>No one algorithm will be suitable for all problems and any algorithm could be the right one.<span class="fragment" data-fragment-index="1">.. <i>but in practice only a handful of algorithms are really worth considering.</i></span></p>

    <div class="row row-eq-height fragment" data-fragment-index="1">
      <div class="col-md-6">
        <div class="well">
          <h4>Random Forests</h4>
        </div>
      </div>
      <div class="col-md-6">
        <div class="well">
          <h4>Neural Networks</h4>
        </div>
      </div>
    </div>
    <div class="row row-eq-height fragment" data-fragment-index="1">
      <div class="col-md-6">
        <div class="well">
          <h4>Support Vector Machines</h4>
        </div>
      </div>
      <div class="col-md-6">
        <div class="well">
          <h4>Cheap Algorithms</h4>
        </div>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There is something called the no-free-lunch theorem, which basically says no one algorithm will be suitable for all problems and any algorithm could be the right one.  But I think some algorithms tend to be much better than others.

      And here they are: Random Forests and Neural Networks are definitely the big two.  Support Vector Machines are also good enough to be worth a shot, and there's no reason not to try various cheap algorithms.
    </aside>
  </section>

  <section id="model-generator">
    <h2>Standard algorithm generator</h2>
    <blockquote>I recommend setting up a reusable generator of starter models</blockquote>
<pre><code class="py" data-trim data-noescape>
def standard_clfs():
  yield ExtraTreesClassifier(n_estimators=256)
  yield Pipeline([
    ('feat', RandomForestClassifier(max_depth=3,
      n_estimators=64, class_weight="auto")),
    ('et', ExtraTreesClassifier(n_estimators=256))])
  yield Pipeline([
    ('scale', StandardScaler()),
    ('svc', SVC())])

for clf in standard_clfs():
  print clf.fit(X_train, y_train).score(X_test, y_test)
</code></pre>
    <aside class="notes">
      I recommend setting up a reusable generator of your starter models.  That way you can quickly and consitently apply a range of models to a problem.
    </aside>
  </section>

  <section id="decision-tree" class="container">
    <div class="row">
      <div class="col-md-6">
        <h2 class="text-left">Decision Trees</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">Easily understood</li>
            <li class="list-group-item">No preprocessing</li>
            <li class="list-group-item">Fast and cheap</li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group">
            <li class="list-group-item">Overfits</li>
            <li class="list-group-item">No mathematical processing</li>
          </ul>
        </div>
      </div>
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.tree import
  DecisionTreeClassifier, DecisionTreeRegressor
</code></pre>
    <aside class="notes" data-markdown>
      Decision trees are arguably the simplest and most intuitive machine learning model in existence.  They are essentially a flowchart: for each node starting at the root, you evaluate a rule and are directed to another node, until you reach a final decision. They require no preprocessing and are fast and cheap to train, store, and evaluate.

      How you actually build a given decision tree from the training data depends on your algorithm. With a simple algorithm and no constraints they can produce the best possible fit to the training data.

      Which is to say, they can easily overfit.  Also, they cannot perform any mathematical processing, so they do not generalize very well.
    </aside>
  </section>

  <section id="random-forest" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Random Forests</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">No preprocessing</li>
            <li class="list-group-item">Minimal tuning</li>
            <li class="list-group-item">Fast</li>
            <li class="list-group-item"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Reliably good results</a></li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group small">
            <li class="list-group-item">No mathematical processing</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.ensemble import
  RandomForestClassifier, RandomForestRegressor,
  ExtraTreesClassifier, ExtraTreesRegressor
</code></pre>
    <aside class="notes" data-markdown>
      A random forest is simply an ensemble of specially constructed decision trees.  Each tree is trained using a bootstraped dataset, and each decision node is based on a random subset of features.  Random forests have several really nice practical aspects:

      - Since they are based on decision trees, no preprocessing of features is necessary.
      - There are few parameters to tune and those parameters are very forgiving.  Essentially it's just the maximum depth, number of features to sample, and the number of trees. Interestingly, adding trees can only improve test accuracy.
      - Training and evaluation is embarassingly parallel, and typically quite fast.  You can add trees for accuracy or remove trees for performance at will.
      - Most importantly random forests work [very well on a very large number of problems](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf).

      In fact if I was really in a pinch and could use only one algorithm, it would be the extremely randomized trees variant, which is an even more random forest.  It will almost certainly give me an accurate result with no fuss whatsoever.

      Forests work well because [bagging is most effective on unstable algorithms that overfit easily](http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf)... and decision trees overfit more easily than just about anything.
    </aside>
  </section>

  <section id="linear-transformation" class="container">
    <h2>Linear Transformations</h2>
    <div class="row">
      <div class="col-md-4 nopadding">
        <br>
        <p>
          $$\begin{align}
          s_j^{(1)} =& \sum_{i} x_{i} w^{(in\rightarrow 1)}_{i\rightarrow j}\\
          S^{(1)} =& X W^{(in\rightarrow 1)}\\
          \end{align}$$
        </p>
      </div>
      <div class="col-md-8">
        <img class="noborder" src="../img/gradient-descent.gif">
      </div>
    </div>

    <aside class="notes" data-markdown>
      The linear family of algorithms are all variations on a simple premise: given an input vector, generate an output vector where every component is a weighted sum of the input vector.  This can be efficiently expressed using matrix algebra, allowing you to process multiple items at a time.

      The key then for any linear algorithm is to find the weight matrix.  This can be done analytically or using iterative methods, most notably gradient descent.
    </aside>
  </section>


  <section id="neural-network" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <svg width="450" height="500"></svg>
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Neural Networks</h2>
        <div class="panel panel-green">
          <ul class="list-group small">
            <li class="list-group-item">Flexible architecture</li>
            <li class="list-group-item">Superior accuracy</li>
            <li class="list-group-item">Learn progressively higher-level features</li>
          </ul>
        </div>
        <div class="panel panel-red">
          <ul class="list-group small">
            <li class="list-group-item">Computationally intensive</li>
            <li class="list-group-item">Long training times</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sknn.mlp import Classifier, Regressor, Layer, Convolution
layers = [Layer("Rectifier", units=100), Layer("Softmax")]
clf = Classifier(layers=layers)
</code></pre>
    <aside class="notes" data-markdown>
      The basic neural network architecture is simply a stack of these linear transformations, where each transformation is wrapped in a non-linear activation function.  Activation functions are required because otherwise the linear transformations could be collapsed down to a single one (which is not very interesting or expressively powerful).

      The layers between the output and input are known as the hidden layers.  In theory a single hidden layer of large enough width could approximate any function, but in practice multiple hidden layers may be much more expressive.  Up until recently it was not possible to train more than a couple of layers, but now there are techniques allowing for many layers.  In addition, variations on the basic architecture such as convolutional layers and recurrent networks are tremendously useful for certain types of problems.
    </aside>
  </section>

  <section id="deep-neural-network" class="container">
    <h2>Deep Neural Networks</h2>
    <img class="noborder" height="250px" width="auto" src="../img/googlenet.png">
    <img class="noborder" height="250px" width="auto" class="fragment" src="../img/weedyseadragon.png">

    <aside class="notes" data-markdown>
      This has led to the renaissance of deep neural networks, which are setting records for processing of structured data like audio and video.  Furthermore, it has been found that these deep networks are actually learning recognizable features at higher and higher levels, to the point that a single node can code for something like "dog".

      However they are not without cost - some of these beasts are absolutely enormous.  This for instance is GoogLeNet: I couldn't even get the model to load without running out of GPU memory on my machine.  Here there be dragons.
    </aside>
  </section>

  <section id="svm" class="container">
    <div class="row">
      <div class="col-md-6 nopadding">
        <img src="../img/svc.gif" style="width:320px;height:auto">
        <img src="../img/svc-projection.gif" style="width:320px;height:auto;clip:rect(0px,140px,140px,0px);">
      </div>
      <div class="col-md-6">
        <h2 class="text-left">Support Vector Machines</h2>
        <div class="panel panel-green small">
          <ul class="list-group">
            <li class="list-group-item">High quality class boundaries</li>
            <li class="list-group-item"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Pretty good accuracy</a></li>
            <li class="list-group-item">One-class training!</li>
          </ul>
        </div>
        <div class="panel panel-red small">
          <ul class="list-group small">
            <li class="list-group-item">Lots of tuning</li>
            <li class="list-group-item">Overfit easily</li>
            <li class="list-group-item">Not suitable for large training sets</li>
          </ul>
        </div>
      </div>
    </div>
<pre><code class="py" data-trim data-noescape>
from sklearn.svm import SVC, SVR
</code></pre>
    <aside class="notes" data-markdown>
      Although random forests and neural networks are the big performers these days, Support Vector Machines are right up there.

      Support Vector Machines exploit something known as the "kernel trick", which is a way of projecting your samples into a higher dimensional combinatorial space.  The trick is that by using certain mathematically behaved functions that work on the dot products between the sample vectors, we don't actually have to *make* the projection to solve the SVM.  The final model after training is based on a selection of the training samples, the so-called support vectors.  Presuming you don't use too many the function is quick to evaluate.

      Support vectors generate very high quality class boundaries and can give good accuracy in many cases.  They also can be trained on a single class for outlier detection.

      But there are big catches.  Tuning the parameters can be quite tricky, as SVMs overfit easily.  And most significantly, the memory requirements are quadratic, which can be a huge burden if you aren't careful.
    </aside>
  </section>

  <section id="new-algorithms">
    <h1>Keep your eyes open!</h1>

    <aside class="notes" data-markdown>
      The state of the art is continually being improved and new algorithms are invented all the time.  So, keep your eyes open.  Follow a machine learning blog, or use news aggregators to see what's new.  You will probably learn a thing or two.
    </aside>
  </section>

</section>

<section>
  <section id="fine-tune">
    <h1 class="text-left">5. Fine tune your best models</h1>
    <div class="row">
      <div class="col-md-6">
        <img class="noborder" src="../img/validation.png">
      </div>
      <div class="col-md-6">
        <ul>
          <li>Try adding or removing features</li>
          <li>Use grid searches to fine-tune hyperparameters</li>
          <li>Train longer</li>
          <li>Make a weighted ensemble of multiple models</li>
        </ul>
      </div>
    </div>

    <aside class="notes" data-markdown>
      Lastly once you have some good models, try improving them. See if adding or removing features makes a difference.  Use a grid search to optimize hyperparameters.  Maybe even make a weighted ensemble of multiple models.
    </aside>
  </section>

  <section id="bias-variance" class="container">
    <h2>The bias-variance tradeoff</h2>
    <div class="row">
      <div class="col-md-6">
        <svg id="bias-target" width="250" height="250">
          <g transform="translate(125, 125) scale(1.23)">
            <g class="target" stroke="black" stroke-width="0.75">
              <circle r="100" style="fill:white;"></circle>
              <circle r="90" style="fill:white;"></circle>
              <circle r="80" style="fill:gray;"></circle>
              <circle r="70" stroke="white" style="fill:gray;"></circle>
              <circle r="60" style="fill:lightblue;"></circle>
              <circle r="50" style="fill:lightblue;"></circle>
              <circle r="40" style="fill:lightcoral;"></circle>
              <circle r="30" style="fill:lightcoral;"></circle>
              <circle r="20" style="fill:yellow;"></circle>
              <circle r="10" style="fill:yellow;"></circle>
            </g>
            <g class="shots" fill="black" stroke="red" stroke-width="0.5"></g>
          </g>
        </svg>
      </div>
      <div class="col-md-6">
        <svg id="variance-target" width="250" height="250">
          <g transform="translate(125, 125) scale(1.23)">
            <g class="target" stroke="black" stroke-width="0.75">
              <circle r="100" style="fill:white;"></circle>
              <circle r="90" style="fill:white;"></circle>
              <circle r="80" style="fill:gray;"></circle>
              <circle r="70" stroke="white" style="fill:gray;"></circle>
              <circle r="60" style="fill:lightblue;"></circle>
              <circle r="50" style="fill:lightblue;"></circle>
              <circle r="40" style="fill:lightcoral;"></circle>
              <circle r="30" style="fill:lightcoral;"></circle>
              <circle r="20" style="fill:yellow;"></circle>
              <circle r="10" style="fill:yellow;"></circle>
            </g>
            <g class="shots" fill="black" stroke="red" stroke-width="0.5"></g>
          </g>
        </svg>
      </div>
    </div>
    <div style="display:none">
      <span class="fragment" id="first-shots"></span>
      <span class="fragment" id="many-shots"></span>
    </div>

    <div class="row">
      <div class="col-md-6">
        <h2>Bias</h2>
      </div>
      <div class="col-md-6">
        <h2>Variance</h2>
      </div>
    </div>

    <aside class="notes" data-markdown>
      There are three sources of error in models **Bias** is the tendency of an model to favor specific representations of the data.  The counterpart of bias is **Variance**, the tendency of an algorithm to find very different representations for small variations in the data.  During training, high bias leads to **Underfitting** because the algorithm is incapable of finding or building a model, while high variance leads to **Overfitting** because it effectively memorizes the training data. What we really want is a model which **Generalizes** the data and can accurately and reliably handle whatever we throw at it.

      Let's use a metaphor. If you were shooting at a target, your bias is how far from the bullseye your shots are centered, and your variance is how spread apart your shots are.  A good model would have a tight grouping right in the center.

      The most important point about the bias-variance tradeoff is this: it is impossible to overcome intrinsic bias in a model, but it *is* possible to overcome variance.  We will discuss some tricks for this, but the most straightforward approach is simply more training with more data on complex, low-bias models. One more reason why "big data" is a big deal.

      Which brings us to the third source of error: insufficient or unrepresentative training data.  Garbage in, garbage out.
    </aside>
  </section>

  <section id="hyperparameter-tuning" class="container">
    <h2>Tuning Hyperparameters</h2>
    <img class="noborder" src="../img/validation.png">

<pre><code class="py small" data-trim data-noescape>
from sklearn.grid_search import GridSearchCV,
from sklearn.cross_validation import StratifiedShuffleSplit

params = {"gamma": np.logspace(-9, 3, 13)}
splits = StratifiedShuffleSplit(y, n_iter=3, test_size=0.2)
grid = GridSearchCV(SVC(), param_grid=params, cv=splits)
grid.fit(X_train, y_train)
print grid.best_params_, grid.best_score_
</code></pre>
    <aside class="notes" data-markdown>
      The bias-variance tradeoff can be adjusted when constructing many models, by adjusting particular hyperparameters.  Choosing the right hyperparameters is very important, and can mean the difference between a model with high accuracy and one that doesn't even train.  As such, you usually need to explore variants of a model with different hyperparameters set.  Tuning hyperparameters can be tricky, as they can interact with each other or be just plain sensitive.

      Here we see an example of hyperparameter tuning for a Support Vector Machine.  The parameter is called gamma and controls how complex the model is allowed to be; we are tuning it across a logarithmic range.  So when gamma is low, the model complexity is low, and therefore the intrinisic bias of the model is high.  Both the training score and the testing score is poor: the model is underfitting.  When gamma is high, the model is allowed to be complex, which means the training score is nearly perfect but the testing score is much worse: the model is overfitting.  The sweet spot is in the middle, at 0.001.  By the way this sort of visual exploration is called a validation curve.

      Of course if when spot-checking your model you use a bad hyperparameter you might not even bother trying to improve it.  So when spot checking models like Support vector machines, do spot checks with widely spaced hyperparameters to see if any makes a difference.

      In python, one would typically use the GridSearchCV to explore a particular parameter space.  You provide the GridSearchCV with the model to tune and a dictionary of parameter options, and it will explore every combination of those options to find the one with the best accuracy.  You can then extract the parameters and retrain the model with the best ones (GridSearchCV will even do that for you).
    </aside>
  </section>

  <section id="performance-data" class="container">
    <h2>Model performance with data</h2>
    <img class="noborder" src="../img/learning.png">

<pre><code class="py small" data-trim data-noescape>
from sklearn.cross_validation import ShuffleSplit
from sklearn.learning_curve import learning_curve

train_sizes = np.linspace(.1, 1.0, 9)
cv = cross_validation.ShuffleSplit(
  len(y), n_iter=10, test_size=0.2)

train_sizes, train_scores, test_scores = learning_curve(
    clf, X, y, cv=cv, train_sizes=train_sizes)
</code></pre>
    <aside class="notes" data-markdown>
      Of course model performance is also heavily dependent on data.  One way to get a feel for the true intrinsic bias of a model is by looking at how well it trains with different amounts of data.  Generally you expect that your train and test accuracy will converge as you train with more and more data.  The shared asymptote of the two curves represents the intrinsic bias, the maximum possible accuracy this model can obtain.  The curve of the two lines gives you an idea about how much data you actually need to get acceptably close to that point.
    </aside>
  </section>

</section>

<section>
  <section id="end" class="container">
    <h2>Thank you!</h2>
    <br>

    <div class="row">
      <div class="col-md-12 text-left">
        <h4>Further reading</h4>
        <ul>
          <li><a href="https://htmlpreview.github.io/?https://raw.githubusercontent.com/jmgore75/PracticalML/master/presentation/conference2015.html">Replay</a> this presentation on <a href="https://github.com/jmgore75/PracticalML/">GitHub</a></li>
          <li><a href="http://machinelearningmastery.com/">Machine learning mastery</a> blog</li>
          <li><a href="https://www.udacity.com/course/machine-learning--ud262">Machine Learning</a> Udacity course series</li>
        </ul>
      </div>
    <div>
  </section>

  <section id="tools">
    <div class="row">
      <div class="col-md-6 text-left">
        <h4>Development Tools</h4>
        <ul>
          <li><a href="https://www.python.org/">Python</a> language</li>
          <li><a href="http://scikit-learn.org">scikit-learn</a> ML library</li>
          <li><a href="https://github.com/aigamedev/scikit-neuralnetwork">scikit-neuralnetwork</a> ANN library</li>
          <li><a href="http://deeplearning.net/software/theano/">Theano</a> numerical library</li>
        </ul>
      </div>
      <div class="col-md-6 text-left">
        <h4>Presentation Tools</h4>
        <ul>
          <li><a href="https://github.com/hakimel/reveal.js">Reveal.js</a> presentation framework</li>
          <li><a href="http://d3js.org/">d3.js</a> charting library</li>
        </ul>
      </div>
    </div>

    <aside class="notes">

    </aside>
  </section>

</section>

</div>
</div>

<!-- Setup -->

<script src="../reveal.js/lib/js/head.min.js"></script>
<script src="../reveal.js/js/reveal.js"></script>

<script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,

    transition: 'concave',

    // Optional reveal.js plugins
    dependencies: [
      { src: '../reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: '../reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: '../reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: '../reveal.js/plugin/math/math.js'},
      { src: '../reveal.js/plugin/highlight/highlight.js', async: true, condition: function() {
          return !!document.querySelector( 'code' );
        }, callback: function() {
          hljs.initHighlightingOnLoad();
        }
      },
      { src: '../reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: '../reveal.js/plugin/notes/notes.js', async: true }
    ]
  });
</script>

<script src="../js/d3.min.js"></script>
<script src="../js/dt.js"></script>
<script>
  //Animations
  (function () {
    var bias_target = new Target(d3.select("#bias-target g.shots"));
    bias_target.shooter = new Shooter(50, Math.PI*1.3, 10);

    var variance_target = new Target(d3.select("#variance-target g.shots"));
    variance_target.shooter = new Shooter(10, Math.PI*0.32, 30);

    Target.prototype.placeShots = function (shot_ids, duration) {
      var shotMap = this.shooter.shots;
      var shots = [];
      shot_ids.forEach(function (shot_id) {
        if (shotMap[shot_id]) {
          shots = shots.concat(shotMap[shot_id]);
        }
      })
      this.doShots(shots, duration);
    }

    Target.prototype.showShots = function () {
      this.doShots(this.shooter.shots["first-shots"]);
    }

    function handleChange(event) {
      var duration = event ? 1000 : 0;
      var cslide = Reveal.getCurrentSlide();
      if (cslide.id === "bias-variance") {
        var vfrag = cslide.querySelectorAll(".fragment.visible");
        var shots = [];
        for (var i = 0; i < vfrag.length; i++) {
          if (vfrag[i].id) {
            shots.push(vfrag[i].id);
          }
        }
        bias_target.placeShots(shots, duration);
        variance_target.placeShots(shots, duration);
      }
    }

    function handleSlide(event) {
      if (event.previousSlide && event.previousSlide.id === "bias-variance") {
        bias_target.showShots();
        variance_target.showShots();
      }
      if (event.currentSlide && event.currentSlide.id === "bias-variance") {
        handleChange();
      }
    }

    Reveal.addEventListener( 'fragmentshown', handleChange);

    Reveal.addEventListener( 'fragmenthidden', handleChange);

    Reveal.addEventListener('ready', handleSlide);

    Reveal.addEventListener( 'slidechanged', handleSlide);

    var graphMap = {
      "title" : [new Tree(5, 3, 0.1), new DAG(7, 3, 8), new NN(4, 8, 8, 8, 3)],
      "decision-tree" : [new Tree(5, 3, 0.2), new Tree(4, 3, 0), new Tree(4, 2, 0.2)],
      "random-forest" : [new Forest(4, 3, 0.1), new Forest(4, 2, 0.2), new Forest(5, 3, 0.2)],
      "decision-jungle" : [new DAG(7, 3, 8), new DAG(8, 2, 6), new DAG(7, 3, 8)],
      "neural-network"  : [new NN(4, 8, 8, 8, 3)],
      "extreme-learning-machines" : [new NN(4, 32, 3)]
    }

    function loopGraphs(slide, graphs) {
      if (!slide) {
        return;
      }
      var graphs = Array.prototype.slice.call(arguments);
      var slide = graphs.shift();

      if (!graphs.length) {
        graphs = graphMap[slide.id] || [];
      }
      var svg = d3.select(slide).select("svg");

      if (svg[0][0] && graphs.length) {
        var t = svg.interrupt().transition();
        svg.selectAll("*").remove();

        function loop(t) {
          graphs.forEach(function (graph) {
            //Update each on a timer
            graph.steps.forEach(function (step) {
              t = t.transition().duration(step.duration || 0).each("start", function () {
                displayStep(step, svg);
              });
            });
            t = t.transition().duration(3000);
            t = t.transition().duration(clearStep.duration || 0).each("start", function () {
              displayStep(clearStep, svg);
            });
            t = t.transition().duration(500).each("start", function () {
              svg.selectAll("*").remove();
            });
          })

          t = t.each("end", function () {
            loop(t);
          });
        }

        loop(t);
      }
    }

    function showGraph(slide, graph) {
      if (!slide) {
        return;
      }
      if (!graph) {
        var graphs = graphMap[slide.id];
        if (graphs) {
          graph = graphs[0];
        }
      }
      var svg = d3.select(slide).select("svg");
      if (svg[0][0] && graph) {
        svg.interrupt().transition();
        svg.selectAll("*").remove();
        var step = graph.lastStep();
        if (step) {
          displayStep(step, svg, 0);
        }
      }
    }

    Reveal.addEventListener('ready', function (event) {
      for (var id in graphMap) {
        var slide = document.getElementById(id);
        if (slide) {
          showGraph(slide)
        }
      }
      loopGraphs(event.currentSlide);
    });

    Reveal.addEventListener( 'slidechanged', function(event) {
      showGraph(event.previousSlide);
      loopGraphs(event.currentSlide);
    } );

  })();

</script>

</body>
</html>
